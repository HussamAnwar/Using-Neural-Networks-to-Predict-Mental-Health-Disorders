# -*- coding: utf-8 -*-
"""MDD.ipynb
Automatically generated by Colab.
"""

import pandas as pd
import numpy as np
import torch
import random as rand
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer, MinMaxScaler
import torch.nn.functional as F
import os

class MDDClassifier(nn.Module):
    def __init__(self, approach, input_dim):
      super().__init__()
      self.approach = approach
      self.input_dim = input_dim
      match approach:
        case "MDD1":
          self.net_stack = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.Dropout(0.7),
            nn.Linear(64, 32),
            nn.Dropout(0.7),
            nn.Linear(32, 1),
            nn.Sigmoid()
          )
        case "MDD2":
          self.net_stack = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.Dropout(0.5),
            nn.Linear(64, 32),
            nn.Dropout(0.5),
            nn.Linear(32, 1),
            nn.Sigmoid()
          )
        case "MDD3":
          self.net_stack = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.Dropout(0.7),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32,32),
            nn.Dropout(0.7),
            nn.Linear(32, 1),
            nn.ReLU(),
            nn.Sigmoid()
          )
        case "MDD4":
          self.net_stack = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.Dropout(0.3),
            nn.Linear(32, 1),
            nn.Sigmoid()
          )
        case "MDD5":
          self.net_stack = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.Dropout(0.5),
            nn.Linear(64, 64),
            nn.Dropout(0.5),
            nn.Linear(64, 32),
            nn.Dropout(0.5),
            nn.Linear(32, 32),
            nn.Dropout(0.5),
            nn.Linear(32, 1),
            nn.Sigmoid()
          )
    def forward(self, x):
        logits = self.net_stack(x)
        return logits
def direct():
  # Step 1: Read the data
  df = pd.read_csv("b_depressed.csv", delimiter=",")

  # Drop irrelevant columns like ID
  df = df.drop(columns=["Survey_id", "Ville_id"])
  
  # Fill missing values (if any)
  #df = df.fillna(0)
  index = list(df)

  # Split features and labels
  X = df.drop(columns=["depressed"])
  y = df["depressed"]
  y = y.to_frame()

  y_array = y.to_numpy()
  count_zero = np.count_nonzero(y_array == 1)

  # Normalize features
  scaler = MinMaxScaler()
  X_scaled = scaler.fit_transform(X)
  X_scaled = np.nan_to_num(X_scaled, nan=0)

  # Convert to PyTorch tensors
  X_tensor = torch.tensor(X_scaled, dtype=torch.float32)
  y_tensor = torch.tensor(y.values, dtype=torch.float32)

  # Split data into train and test sets
  X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2)


  # Define weights initialization function
  def weights_init_normal(m):
      classname = m.__class__.__name__
      if classname.find('Linear') != -1:
          torch.nn.init.normal_(m.weight.data, 0.0, 0.1)
          torch.nn.init.constant_(m.bias.data, 0)

  def weights_init_xavier(m):
      classname = m.__class__.__name__
      if classname.find('Linear') != -1:
          torch.nn.init.xavier_uniform_(m.weight.data)
          torch.nn.init.constant_(m.bias.data, 0)

  def weights_init_he(m):
      classname = m.__class__.__name__
      if classname.find('Linear') != -1:
          torch.nn.init.kaiming_uniform_(m.weight.data, a=0, mode='fan_in', nonlinearity='relu')
          torch.nn.init.constant_(m.bias.data, 0)

  def train_model(model, criterion, patience, model_path):
      # Initialize the model with weights initialized normally
      input_dim = X_train.shape[1]
      model.apply(weights_init_normal)  # Apply weight initialization to the model
      # Initialize the optimizer
      optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)

      # Variables for early stopping
      early_stop_counter = 0
      best_val_loss = float('inf')
      best_model_state = None

      # Training loop
      for epoch in range(100):
          # Shuffle the training set
          indices = torch.randperm(X_train.size(0))
          X_train_shuffled = X_train[indices]
          y_train_shuffled = y_train[indices]

          # Set model to training mode
          model.train()

          # Forward pass
          outputs = model(X_train_shuffled)
          loss = criterion(outputs, y_train_shuffled.view(-1, 1))

          # Backward pass and optimization
          optimizer.zero_grad()
          loss.backward()
          optimizer.step()

          # Calculate training accuracy
          train_accuracy = ((outputs >= 0.5).float() == y_train_shuffled.view(-1, 1)).float().mean()

          # Set model to evaluation mode
          model.eval()

          # Validation
          with torch.no_grad():
              val_outputs = model(X_test)
              val_loss = criterion(val_outputs, y_test.view(-1, 1))

          # Check for early stopping
          if val_loss < best_val_loss:
              best_val_loss = val_loss
              best_model_state = model.state_dict()
              early_stop_counter = 0
          #remove the below apostrophes to implement early stopping.
          '''else: 
              early_stop_counter += 1
              if val_loss < best_val_loss:
                best_model_state = model.state_dict()
              if early_stop_counter >= patience:
                  print(f"Early stopping at epoch {epoch+1} with validation loss: {val_loss.item():.4f} and Training Accuracy: {train_accuracy.item():.4f}")
                  break
          '''
          #print(f"Epoch [{epoch+1}/100], Loss: {loss.item():.4f}, Training Accuracy: {train_accuracy.item():.4f}, Validation Loss: {val_loss.item():.4f}")
      print(f"Training Accuracy for {model.approach}: {train_accuracy.item():.4f}")

      return best_model_state

  # Define loss function
  criterion = nn.BCELoss()

  # Define patience for early stopping
  patience = 5

  # Lists to store results of multiple training runs
  test_accuracies = []
  num_correct_predictions = []

  model_num = 0
  directory = "CAPMODELS"
  out_path = directory + "/" +"output"
  os.makedirs(out_path, exist_ok=True)

  approach_list = ["MDD1", "MDD2", "MDD3", "MDD4", "MDD5"]
  for approach in approach_list:
    input_dim = X_train.shape[1]
    model = MDDClassifier(approach, input_dim)
    model_path = os.path.join(out_path, "model_" + approach + ".pth")

    trained_model = train_model(model, criterion, patience, model_path)
    torch.save(trained_model, model_path)

    best_model_state = model.load_state_dict(torch.load(model_path))
    for i in range(0,1):
      # Test the model
      with torch.no_grad():
          test_outputs = torch.round(model(X_test))
          test_accuracy = (test_outputs == y_test.view(-1, 1)).float().mean()
          print(f"Testing Accuracy: {test_accuracy.item():.4f}" + " " + approach)
          # Calculate the number of correct predictions
          correct_predictions = torch.sum(test_outputs == y_test.view(-1, 1)).item()
          print(f"Number of correct predictions for model_" + approach + ":" + f"{correct_predictions}/{len(y_test)}")
          for i in range(len(y_test)):
              if test_outputs[i].item() != y_test[i].item():
                  print(f"ID: {df.index[i]}, Predicted: {test_outputs[i].item()}, Actual: {y_test[i].item()}")
          # Append results to lists
          test_accuracies.append(test_accuracy.item())
          num_correct_predictions.append(correct_predictions)

if __name__ == "__main__":
  direct()
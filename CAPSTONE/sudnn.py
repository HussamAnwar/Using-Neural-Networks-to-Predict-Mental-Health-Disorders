# -*- coding: utf-8 -*-
"""SUDNN.ipynb
Automatically generated by Colab.
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import os


class SUDClassifier(nn.Module):
    def __init__(self, approach, input_dim):
      super().__init__()
      self.approach = approach
      self.input_dim = input_dim
      match approach:
        case "SUD1":
          self.net_stack = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.Dropout(0.7),
            nn.Linear(64, 32),
            nn.Dropout(0.7),
            nn.Linear(32, 1),
            nn.Sigmoid()
          )
        case "SUD2":
          self.net_stack = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.Dropout(0.5),
            nn.Linear(64, 32),
            nn.Dropout(0.5),
            nn.Linear(32, 1),
            nn.Sigmoid()
          )
        case "SUD3":
          self.net_stack = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.Dropout(0.7),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32,32),
            nn.Dropout(0.7),
            nn.Linear(32, 1),
            nn.ReLU(),
            nn.Sigmoid()
          )
        case "SUD4":
          self.net_stack = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.Dropout(0.3),
            nn.Linear(32, 1),
            nn.Sigmoid()
          )
        case "SUD5":
          self.net_stack = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.Dropout(0.5),
            nn.Linear(64, 64),
            nn.Dropout(0.5),
            nn.Linear(64, 32),
            nn.Dropout(0.5),
            nn.Linear(32, 32),
            nn.Dropout(0.5),
            nn.Linear(32, 1),
            nn.Sigmoid()
          )
    def forward(self, x):
        logits = self.net_stack(x)
        return logits
def direct():

  #Read the data
  df = pd.read_csv("patient_info.csv", delimiter=";")

  # Drop irrelevant columns like ID
  df = df.drop(columns=["ID", "HRV_TIME", "ACC_TIME", "ACC_DAYS", "ACC", "HRV_HOURS", "HRV", "SEX", "filter_$"])

  #Preprocess the data
  
  # Fill missing values
  df = df.fillna(0)
  df["SUBSTANCE"] = df["SUBSTANCE"].replace(2,1)
  df["SUBSTANCE"] = df["SUBSTANCE"].replace(9,1)
  
  # Split features and labels
  X = df.drop(columns=["SUBSTANCE"])
  y = df["SUBSTANCE"]

  # Normalize features
  scaler = StandardScaler()
  X_scaled = scaler.fit_transform(X)

  # Convert to PyTorch tensors
  X_tensor = torch.tensor(X_scaled, dtype=torch.float32)
  y_tensor = torch.tensor(y.values, dtype=torch.float32)

  # Split data into train and test sets
  X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2)

  # Define weights initialization function
  def weights_init_normal(m):
      classname = m.__class__.__name__
      if classname.find('Linear') != -1:
          torch.nn.init.normal_(m.weight.data, 0.0, 0.1)
          torch.nn.init.constant_(m.bias.data, 0)

  def weights_init_xavier(m):
      classname = m.__class__.__name__
      if classname.find('Linear') != -1:
          torch.nn.init.xavier_uniform_(m.weight.data)
          torch.nn.init.constant_(m.bias.data, 0)

  def weights_init_he(m):
      classname = m.__class__.__name__
      if classname.find('Linear') != -1:
          torch.nn.init.kaiming_uniform_(m.weight.data, a=0, mode='fan_in', nonlinearity='relu')
          torch.nn.init.constant_(m.bias.data, 0)

  def train_model(model, criterion, patience, model_path):
      # Initialize the model with weights initialized normally
      input_dim = X_train.shape[1]

      model.apply(weights_init_normal)  # Apply weight initialization to the model
      # Initialize the optimizer
      optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)

      # Variables for early stopping
      early_stop_counter = 0
      best_val_loss = float('inf')
      best_model_state = None

      # Training loop
      for epoch in range(100):
          # Shuffle the training set
          indices = torch.randperm(X_train.size(0))
          X_train_shuffled = X_train[indices]
          y_train_shuffled = y_train[indices]

          # Set model to training mode
          model.train()

          # Forward pass
          outputs = model(X_train_shuffled)
          loss = criterion(outputs, y_train_shuffled.view(-1, 1))

          # Backward pass and optimization
          optimizer.zero_grad()
          loss.backward()
          optimizer.step()

          # Calculate training accuracy
          train_accuracy = ((outputs >= 0.5).float() == y_train_shuffled.view(-1, 1)).float().mean()

          # Set model to evaluation mode
          model.eval()

          # Validation
          with torch.no_grad():
              val_outputs = model(X_test)
              val_loss = criterion(val_outputs, y_test.view(-1, 1))

          # Check for early stopping
          if val_loss < best_val_loss:
              best_val_loss = val_loss
              best_model_state = model.state_dict()
              early_stop_counter = 0
          #remove the below apostrophes to implement early stopping.
          '''else: 
              early_stop_counter += 1
              if val_loss < best_val_loss:
                best_model_state = model.state_dict()
              if early_stop_counter >= patience:
                  print(f"Early stopping at epoch {epoch+1} with validation loss: {val_loss.item():.4f} and Training Accuracy: {train_accuracy.item():.4f}")
                  break
          '''
          #print(f"Epoch [{epoch+1}/100], Loss: {loss.item():.4f}, Training Accuracy: {train_accuracy.item():.4f}, Validation Loss: {val_loss.item():.4f}")
      print(f"Training Accuracy for {model.approach}: {train_accuracy.item():.4f}")

      return best_model_state

  # Define loss function
  criterion = nn.BCELoss()

  # Define patience for early stopping
  patience = 5

  # Lists to store results of multiple training runs
  test_accuracies = []
  num_correct_predictions = []

  model_num = 0
  directory = "CAPMODELS"
  out_path = directory + "/" +"output"
  os.makedirs(out_path, exist_ok=True)

  approach_list = ["SUD1", "SUD2", "SUD3", "SUD4", "SUD5"]
  for approach in approach_list:
    input_dim = X_train.shape[1]
    model = SUDClassifier(approach, input_dim)
    model_path = os.path.join(out_path, "model_" + approach + ".pth")


    trained_model = train_model(model, criterion, patience, model_path)
    torch.save(trained_model, model_path)

    best_model_state = model.load_state_dict(torch.load(model_path))
    for i in range(0,1):
      # Test the model
      with torch.no_grad():
          test_outputs = torch.round(model(X_test))
          test_accuracy = (test_outputs == y_test.view(-1, 1)).float().mean()
          print(f"Testing Accuracy: {test_accuracy.item():.4f}" + " " + approach)
          # Calculate the number of correct predictions
          correct_predictions = torch.sum(test_outputs == y_test.view(-1, 1)).item()
          print(f"Number of correct predictions for model_" + approach + ":" + f"{correct_predictions}/{len(y_test)}")
          for i in range(len(y_test)):
              if test_outputs[i].item() != y_test[i].item():
                  print(f"ID: {df.index[i]}, Predicted: {test_outputs[i].item()}, Actual: {y_test[i].item()}")
          # Append results to lists
          test_accuracies.append(test_accuracy.item())
          num_correct_predictions.append(correct_predictions)

  # Calculate average results
  avg_test_accuracy = sum(test_accuracies) / len(test_accuracies)
  avg_num_correct_predictions = sum(num_correct_predictions) / len(num_correct_predictions)
  print(f"Average Testing Accuracy: {avg_test_accuracy:.4f}")
  print(f"Average Number of Correct Predictions: {avg_num_correct_predictions:.2f}")


if __name__ == "__main__":
  direct()
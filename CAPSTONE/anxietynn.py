# -*- coding: utf-8 -*-
"""AnxietyNN.ipynb
Automatically generated by Colab.
"""

import pandas as pd
import numpy as np
import torch
import random as rand
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer, MinMaxScaler
import torch.nn.functional as F
from tqdm import tqdm
import os


class GADClassifier(nn.Module):
    def __init__(self, approach, input_dim):
      super().__init__()
      self.approach = approach
      self.input_dim = input_dim
      match approach:
        case "GAD1":
          self.net_stack = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.Dropout(0.7),
            nn.Linear(64, 32),
            nn.Dropout(0.7),
            nn.Linear(32, 1),
            nn.Sigmoid()
          )
        case "GAD2":
          self.net_stack = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.Dropout(0.5),
            nn.Linear(64, 32),
            nn.Dropout(0.5),
            nn.Linear(32, 1),
            nn.Sigmoid()
          )
        case "GAD3":
          self.net_stack = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.Dropout(0.7),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32,32),
            nn.Dropout(0.7),
            nn.Linear(32, 1),
            nn.ReLU(),
            nn.Sigmoid()
          )
        case "GAD4":
          self.net_stack = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.Dropout(0.3),
            nn.Linear(32, 1),
            nn.Sigmoid()
          )
        case "GAD5":
          self.net_stack = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.Dropout(0.5),
            nn.Linear(64, 64),
            nn.Dropout(0.5),
            nn.Linear(64, 32),
            nn.Dropout(0.5),
            nn.Linear(32, 32),
            nn.Dropout(0.5),
            nn.Linear(32, 1),
            nn.Sigmoid()
          )
    def forward(self, x):
        logits = self.net_stack(x)
        return logits
def direct():
  df = pd.read_csv("GamingStudy_data.csv", delimiter=",", encoding = 'unicode_escape')

  # Drop irrelevant columns like ID
  df = df.drop(columns=["GADE", "S. No.", "Timestamp", "Game", "Platform", "earnings", "whyplay", "League", "highestleague", "Work", "Degree", "Gender", "Birthplace", "Residence", "Reference", "Playstyle", "accept", "Residence_ISO3", "Birthplace_ISO3"])
  print(df)

  # Fill missing values (if any)
  df = df.fillna(0)
  index = list(df)

  print(index)
  GAD7S = df["GAD_T"]

  #checking the values to see if an individual scored for moderate anxiety or above.
  GAD7S = GAD7S.to_frame()
  df = df.assign(Anxiety = GAD7S.values)
  df.loc[df['Anxiety'] < 10, 'Anxiety'] = 0
  df.loc[df['Anxiety'] >= 10, 'Anxiety'] = 1

  X = df.drop(columns=["Anxiety"])
  y = df["Anxiety"]
  y = y.to_frame()
  print(df)

  print(y)
  y_array = y.to_numpy()
  count_zero = np.count_nonzero(y_array == 1)
  print(count_zero)

  # Normalize features
  print(df)
  scaler = StandardScaler()
  X_scaled = scaler.fit_transform(X)
  X_scaled = np.nan_to_num(X_scaled, nan=0)

  #X_scaled = np.array(X_scaled)
  #np.set_printoptions(threshold=np.inf)
  #print(X_scaled)


  # Convert to PyTorch tensors
  X_tensor = torch.tensor(X_scaled, dtype=torch.float32)
  y_tensor = torch.tensor(y.values, dtype=torch.float32)
  #torch.set_printoptions(profile="full")
  #print(X_tensor)
  #print(y_tensor)

  # Split data into train and test sets
  X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2)
  #torch.set_printoptions(profile="full")
  #print(X_train)
  #print(y_train)
  #print(df.to_markdown())

  # Define weights initialization function
  def weights_init_normal(m):
      classname = m.__class__.__name__
      if classname.find('Linear') != -1:
          torch.nn.init.normal_(m.weight.data, 0.0, 0.1)
          torch.nn.init.constant_(m.bias.data, 0)

  def weights_init_xavier(m):
      classname = m.__class__.__name__
      if classname.find('Linear') != -1:
          torch.nn.init.xavier_uniform_(m.weight.data)
          torch.nn.init.constant_(m.bias.data, 0)

  def weights_init_he(m):
      classname = m.__class__.__name__
      if classname.find('Linear') != -1:
          torch.nn.init.kaiming_uniform_(m.weight.data, a=0, mode='fan_in', nonlinearity='relu')
          torch.nn.init.constant_(m.bias.data, 0)

  def train_model(model, criterion, patience, model_path):
    #for _ in range(20):
      # Initialize the model with weights initialized normally
      input_dim = X_train.shape[1]
      #model = ADHDClassifier(input_dim)
      model.apply(weights_init_normal)  # Apply weight initialization to the model
      # Initialize the optimizer
      optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)

      # Variables for early stopping
      early_stop_counter = 0
      best_val_loss = float('inf')
      best_model_state = None

      # Training loop
      for epoch in tqdm(range(100)):
          # Shuffle the training set
          indices = torch.randperm(X_train.size(0))
          X_train_shuffled = X_train[indices]
          y_train_shuffled = y_train[indices]

          # Set model to training mode
          model.train()

          # Forward pass
          outputs = model(X_train_shuffled)
          loss = criterion(outputs, y_train_shuffled.view(-1, 1))

          # Backward pass and optimization
          optimizer.zero_grad()
          loss.backward()
          optimizer.step()

          # Calculate training accuracy
          train_accuracy = ((outputs >= 0.5).float() == y_train_shuffled.view(-1, 1)).float().mean()

          # Set model to evaluation mode
          model.eval()

          # Validation
          with torch.no_grad():
              val_outputs = model(X_test)
              val_loss = criterion(val_outputs, y_test.view(-1, 1))

          # Check for early stopping
          if val_loss < best_val_loss:
              best_val_loss = val_loss
              best_model_state = model.state_dict()
              early_stop_counter = 0
          #remove the below apostrophes to implement early stopping.
          '''else: 
              early_stop_counter += 1
              if val_loss < best_val_loss:
                best_model_state = model.state_dict()
              if early_stop_counter >= patience:
                  print(f"Early stopping at epoch {epoch+1} with validation loss: {val_loss.item():.4f} and Training Accuracy: {train_accuracy.item():.4f}")
                  break
          '''
          #print(f"Epoch [{epoch+1}/100], Loss: {loss.item():.4f}, Training Accuracy: {train_accuracy.item():.4f}, Validation Loss: {val_loss.item():.4f}")
      print(f"Training Accuracy for {model.approach}: {train_accuracy.item():.4f}")

      return best_model_state

  # Define loss function
  criterion = nn.BCELoss()

  # Define patience for early stopping
  patience = 5

  # Lists to store results of multiple training runs
  test_accuracies = []
  num_correct_predictions = []

  model_num = 0
  directory = "CAPMODELS"
  out_path = directory + "/" +"output"
  os.makedirs(out_path, exist_ok=True)

  approach_list = ["GAD1", "GAD2", "GAD3", "GAD4", "GAD5"]
  for approach in approach_list:
    input_dim = X_train.shape[1]
    model = GADClassifier(approach, input_dim)
    model_path = os.path.join(out_path, "model_" + approach + ".pth")

    trained_model = train_model(model, criterion, patience, model_path)
    torch.save(trained_model, model_path)

    best_model_state = model.load_state_dict(torch.load(model_path))
    for i in range(0,1):
      # Test the model
      with torch.no_grad():
          test_outputs = torch.round(model(X_test))
          test_accuracy = (test_outputs == y_test.view(-1, 1)).float().mean()
          print(f"Testing Accuracy: {test_accuracy.item():.4f}" + " " + approach)
          # Calculate the number of correct predictions
          correct_predictions = torch.sum(test_outputs == y_test.view(-1, 1)).item()
          print(f"Number of correct predictions for model_" + approach + ":" + f"{correct_predictions}/{len(y_test)}")
          for i in range(len(y_test)):
              '''#Delete these apostrophes to see which ID's were wrongly precicted
              if test_outputs[i].item() != y_test[i].item():
                  print(f"ID: {df.index[i]}, Predicted: {test_outputs[i].item()}, Actual: {y_test[i].item()}")
              '''
          # Append results to lists
          test_accuracies.append(test_accuracy.item())
          num_correct_predictions.append(correct_predictions)

if __name__ == "__main":
  direct()